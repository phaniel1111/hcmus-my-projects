{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSYvP0n8RjYH"
   },
   "source": [
    "# Lab02: Frequent itemset mining\n",
    "\n",
    "- Student ID: 20120313\n",
    "- Student name: Phan Tấn Kiệt\n",
    "\n",
    "**How to do your homework**\n",
    "\n",
    "\n",
    "You will work directly on this notebook; the word `TODO` indicate the parts you need to do.\n",
    "\n",
    "You can discuss ideas with classmates as well as finding information from the internet, book, etc...; but *this homework must be your*.\n",
    "\n",
    "**How to submit your homework**\n",
    "\n",
    "Before submitting, rerun the notebook (`Kernel` ->` Restart & Run All`).\n",
    "\n",
    "Then create a folder named `ID` (for example, if your ID is 1234567, then name the folder `1234567`) Copy file notebook to this folder, compress and submit it on moodle.\n",
    "\n",
    "**Contents:**\n",
    "\n",
    "- Frequent itemset mining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXZ5gCVaRjYa"
   },
   "source": [
    "# 1. Preliminaries\n",
    "## This is how it all started ...\n",
    "- Rakesh Agrawal, Tomasz Imielinski, Arun N. Swami: Mining Association Rules between Sets of Items in Large Databases. SIGMOD Conference 1993: 207-216\n",
    "- Rakesh Agrawal, Ramakrishnan Srikant: Fast Algorithms for Mining Association Rules in Large Databases. VLDB 1994: 487-499\n",
    "\n",
    "**These two papers are credited with the birth of Data Mining**\n",
    "## Frequent itemset mining (FIM)\n",
    "\n",
    "Find combinations of items (itemsets) that occur frequently.\n",
    "## Applications\n",
    "- Items = products, transactions = sets of products someone bought in one trip to the store.\n",
    "$\\Rightarrow$ items people frequently buy together.\n",
    "    + Example: if people usually buy bread and coffee together, we run a sale of bread to attract people attention and raise price of coffee.\n",
    "- Items = webpages, transactions = words. Unusual words appearing together in a large number of documents, e.g., “Brad” and “Angelina,” may indicate an interesting relationship.\n",
    "- Transactions = Sentences, Items = Documents containing those sentences. Items that appear together too often could represent plagiarism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8vAJ8A2RjYi"
   },
   "source": [
    "## Transactional Database\n",
    "A transactional database $D$ consists of $N$ transactions: $D=\\left\\{T_1,T_2,...,T_N\\right\\}$. A transaction $T_n \\in D (1 \\le n \\le N)$ contains one or more items and that $I= \\left\\{ i_1,i_2,…,i_M \\right\\}$ is the set of distinct items in $D$, $T_n \\subset I$. Commonly, a transactional database is represented by a flat file instead of a database system: items are non-negative integers, each row represents a transaction, items in a transaction separated by space.\n",
    "\n",
    "Example: \n",
    "\n",
    "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 \n",
    "\n",
    "30 31 32 \n",
    "\n",
    "33 34 35 \n",
    "\n",
    "36 37 38 39 40 41 42 43 44 45 46 \n",
    "\n",
    "38 39 47 48 \n",
    "\n",
    "38 39 48 49 50 51 52 53 54 55 56 57 58 \n",
    "\n",
    "32 41 59 60 61 62 \n",
    "\n",
    "3 39 48 \n",
    "\n",
    "63 64 65 66 67 68 \n",
    "\n",
    "\n",
    "\n",
    "# Definition\n",
    "\n",
    "- Itemset: A collection of one or more items.\n",
    "    + Example: {1 4 5}\n",
    "- **k-itemset**: An itemset that contains k items.\n",
    "- Support: Frequency of occurrence of an itemset.\n",
    "    + Example: From the example above, item 3 appear in 2 transactions so its support is 2.\n",
    "- Frequent itemset: An itemset whose support is greater than or equal to a `minsup` threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdykKxr6RjY-"
   },
   "source": [
    "# The Apriori Principle\n",
    "- If an itemset is frequent, then all of its subsets must also be frequent.\n",
    "- If an itemset is not frequent, then all of its supersets cannot be frequent.\n",
    "- The support of an itemset never exceeds the support of its subsets.\n",
    "$$ \\forall{X,Y}: (X \\subseteq Y) \\Rightarrow s(X)\\ge s(Y)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvfMR7-CRjZB"
   },
   "source": [
    "# 2. Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9gZh4DORjZD"
   },
   "source": [
    "## The Apriori algorithm\n",
    "Suppose:\n",
    "\n",
    "$C_k$ candidate itemsets of size k.\n",
    "\n",
    "$L_k$ frequent itemsets of size k.\n",
    "\n",
    "The level-wise approach of Apriori algorithm can be descibed as follow:\n",
    "1. k=1, $C_k$ = all items.\n",
    "2. While $C_k$ not empty:\n",
    "    3. Scan the database to find which itemsets in $C_k$ are frequent and put them into $L_k$.\n",
    "    4. Use $L_k$ to generate a collection of candidate itemsets $C_{k+1}$ of size k+1.\n",
    "    5. k=k+1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qF9xHOBLRjZJ"
   },
   "source": [
    "### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7F0lUOSuRjZN"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OogwdcLRjZf"
   },
   "source": [
    "### Read data\n",
    "First we have to read data from database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSTC78WURjZu"
   },
   "source": [
    "### Tree Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "U2bsGrTERjZg"
   },
   "outputs": [],
   "source": [
    "\n",
    "def readData(path):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------\n",
    "        path: path of database D.\n",
    "         \n",
    "    --------------------------\n",
    "    Returns\n",
    "        data: a dictionary for representing database D\n",
    "                 - keys: transaction tids\n",
    "                 - values: itemsets.\n",
    "        s: support of distict items in D.\n",
    "    \"\"\"\n",
    "    data={}\n",
    "    s=defaultdict(lambda: 0) # Initialize a dictionary for storing support of items in I.  \n",
    "    with open(path,'rt') as f:\n",
    "        tid=1;\n",
    "        for line in f:\n",
    "            itemset=set(map(int,line.split())) # a python set is a native way for storing an itemset.\n",
    "            for item in itemset:  \n",
    "                s[item]+=1     #Why don't we compute support of items while reading data?\n",
    "            data[tid]= itemset\n",
    "            tid+=1\n",
    "    \n",
    "    return data, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGAkmuXtRjZw"
   },
   "source": [
    "**I gave you pseudo code of Apriori algorithm above but we implement Tree Projection. Tell me the differences of two algorithms.**\n",
    "\n",
    "\n",
    "**TODO:**  \n",
    "Apriori sử dụng phương pháp kết hợp và cắt bỏ các tập hợp ứng viên dựa trên giá trị support. Thuật toán này sử dụng cấu trúc candidate itemsets để lưu trữ các tập hợp ứng viên và tiếp tục sinh ra các frequent itemsets cho đến khi không còn tập hợp nào thỏa mãn điều kiện support nữa.  \n",
    "  \n",
    "Tree Projection là một phương pháp cải tiến của Apriori. Thuật toán này sử dụng cấu trúc cây để lưu trữ dữ liệu dạng transaction và xây dựng các frequent itemsets. Các frequent itemsets được sinh ra từ cây mà không cần phải thực hiện quá nhiều các phép kết hợp và cắt bỏ như trong Apriori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BVRT5BnWRjZz"
   },
   "outputs": [],
   "source": [
    "def joinset(a, b):\n",
    "    '''\n",
    "    Parameters\n",
    "    -------------------\n",
    "        2 itemsets a and b (of course they are at same branch in search space)\n",
    "\n",
    "    -------------------\n",
    "    return\n",
    "        ret: itemset generated by joining a and b\n",
    "    '''\n",
    "    # TODO (hint: this function will be called in generateSearchSpace method.):\n",
    "    sp =0\n",
    "    t1= a.copy()\n",
    "    t2 = b.copy()\n",
    "    for itemset in data.items():\n",
    "        if set(a['itemset']+b['itemset']).issubset(itemset[1]):\n",
    "            sp+=1\n",
    "    t2['itemset'] = sorted(list(set(t1['itemset']+t2['itemset'])))\n",
    "    t2['pruned'] = False\n",
    "    t2['support'] = sp\n",
    "    ret = t1  \n",
    "    difference = list(set(t2['itemset'])-set(t1['itemset']))\n",
    "    ret[difference[0]] = t2\n",
    "    return ret\n",
    "\n",
    "class TP:\n",
    "    def __init__(self, data=None, s=None, minSup=None):\n",
    "        self.data = data\n",
    "        self.s = {}\n",
    "\n",
    "        for key, support in sorted(s.items(), key=lambda item: item[1]):\n",
    "            self.s[key] = support\n",
    "        # TODO: why should we do this, answer it at the markdown below?\n",
    "        self.minSup = minSup\n",
    "        self.L = {}  # Store frequent itemsets mined from database\n",
    "        self.runAlgorithm()\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"\n",
    "        Initialize search space at first step\n",
    "        --------------------------------------\n",
    "        We represent our search space in a tree structure\n",
    "        \"\"\"\n",
    "        tree = {}\n",
    "        search_space = {}\n",
    "        for item, support in self.s.items():\n",
    "            search_space[item] = {}\n",
    "            search_space[item]['itemset'] = [item]\n",
    "            ''' \n",
    "            python set does not remain elements order\n",
    "            so we use a list to extend it easily when create new itemset \n",
    "            but why we store itemset in data by a python set???? '''\n",
    "            # TODO: study about python set and its advantages,\n",
    "            # answer at the markdown below.\n",
    "            search_space[item]['pruned'] = False\n",
    "            # TODO:\n",
    "            # After finish implementing the algorithm tell me why should you use this\n",
    "            # instead of delete item directly from search_space and tree.\n",
    "            search_space[item]['support'] = support\n",
    "\n",
    "            tree[item] = {}\n",
    "            '''\n",
    "            Why should i store an additional tree (here it called tree)? \n",
    "            Answer: This really help in next steps.\n",
    "\n",
    "            Remember that there is always a big gap from theory to practicality\n",
    "            and implementing this algorithm in python is not as simple as you think.\n",
    "            '''\n",
    "        return tree, search_space\n",
    "\n",
    "    def computeItemsetSupport(self, itemset):\n",
    "        '''Return support of itemset'''\n",
    "        # TODO (hint: this is why i use python set in data)\n",
    "        return itemset['support']\n",
    "    \n",
    "\n",
    "    def get_sub_tree(self, k, tree, search_space, itter_node):\n",
    "        if k == 0:\n",
    "            return search_space[itter_node]['support']\n",
    "        subtree = search_space[itter_node]\n",
    "        for node in subtree.keys():\n",
    "            k-=1\n",
    "            self.get_sub_tree(k,tree,search_space,node)\n",
    "\n",
    "\n",
    "    def prune(self, k, tree, search_space):\n",
    "\n",
    "        '''\n",
    "        In this method we will find out which itemset in current search space is frequent\n",
    "        itemset then add it to L[k]. In addition, we prune those are not frequent itemsets.\n",
    "        '''\n",
    "        if self.L.get(k) is None: self.L[k] = []\n",
    "        # TODO\n",
    "        frequent_itemsets = {}\n",
    "        non_frequent_itemsets = {}\n",
    "        for itemset in search_space.items():\n",
    "            # Kiểm tra độ support của frequent itemset\n",
    "            support = self.computeItemsetSupport(itemset[1].copy())\n",
    "            if support >= self.minSup:\n",
    "                frequent_itemsets[itemset[0]] = itemset[1].copy()\n",
    "            else:\n",
    "                non_frequent_itemsets[itemset[0]] = itemset[1].copy()\n",
    "        # Thêm các tập phổ biến vào danh sách L[k]\n",
    "        temp =[]\n",
    "        for itemset in frequent_itemsets.values():\n",
    "            self.L[k].append(itemset['itemset'].copy())\n",
    "        # Cắt các tập không phổ biến trong không gian tìm kiếm\n",
    "        for itemset in non_frequent_itemsets:\n",
    "            search_space[itemset]['pruned'] =True\n",
    "\n",
    "    def generateSearchSpace(self, k, tree, search_space):\n",
    "        '''\n",
    "        Generate search space for exploring k+1 itemset. (Recursive function)\n",
    "        '''\n",
    "        items = list(tree.keys())\n",
    "        ''' print search_space.keys() you will understand  \n",
    "         why we need an additional tree, '''\n",
    "        l = len(items)\n",
    "        self.prune(k, tree, search_space)\n",
    "        if l == 0: return  # Stop condition\n",
    "        for i in range(l - 1):\n",
    "            sub_search_space = {}\n",
    "            sub_tree = {}\n",
    "            a = items[i]\n",
    "            if search_space[a]['pruned']: continue\n",
    "            for j in range(i + 1, l):\n",
    "                b = items[j]\n",
    "                search_space[a][b] = {}\n",
    "                tree[a][b] = {}\n",
    "                # You really need to understand what am i doing here before doing work below.\n",
    "                # (Hint: draw tree and search space to draft).\n",
    "                # TODO:\n",
    "                # First create newset using join set\n",
    "                newset = joinset(search_space[a],search_space[b])\n",
    "                # Second add newset to search_space\n",
    "                search_space[a]=newset\n",
    "                sub_search_space[b] = search_space[a][b]\n",
    "                sub_tree[b] ={}\n",
    "            #  Generate search_space for k+1-itemset\n",
    "            self.generateSearchSpace(k+1, sub_tree, sub_search_space)\n",
    "                \n",
    "\n",
    "    def runAlgorithm(self):\n",
    "        tree, search_space = self.initialize()  # generate search space for 1-itemset\n",
    "        self.generateSearchSpace(1, tree, search_space)\n",
    "\n",
    "    def miningResults(self):\n",
    "        return self.L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tMTpwxLRjZ-"
   },
   "source": [
    "Ok, let's test on a typical dataset `chess`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "gLygYqiYRjZ-"
   },
   "outputs": [],
   "source": [
    "data, s= readData('chess.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PnxbU77YRjaF",
    "outputId": "c3b158be-6b46-4a3c-9b71-6a92d3d31ded",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: [[48], [56], [66], [34], [62], [7], [36], [60], [40], [29], [52], [58]], 2: [[48, 52], [48, 58], [29, 56], [52, 56], [56, 58], [60, 66], [29, 66], [52, 66], [58, 66], [34, 40], [29, 34], [34, 52], [34, 58], [60, 62], [40, 62], [29, 62], [52, 62], [58, 62], [7, 60], [7, 40], [7, 29], [7, 52], [7, 58], [36, 60], [36, 40], [29, 36], [36, 52], [36, 58], [40, 60], [29, 60], [52, 60], [58, 60], [29, 40], [40, 52], [40, 58], [29, 52], [29, 58], [52, 58]], 3: [[48, 52, 58], [29, 52, 56], [29, 56, 58], [52, 56, 58], [29, 60, 66], [52, 60, 66], [58, 60, 66], [29, 52, 66], [29, 58, 66], [52, 58, 66], [29, 34, 40], [34, 40, 52], [34, 40, 58], [29, 34, 52], [29, 34, 58], [34, 52, 58], [29, 60, 62], [52, 60, 62], [58, 60, 62], [29, 40, 62], [40, 52, 62], [40, 58, 62], [29, 52, 62], [29, 58, 62], [52, 58, 62], [7, 40, 60], [7, 29, 60], [7, 52, 60], [7, 58, 60], [7, 29, 40], [7, 40, 52], [7, 40, 58], [7, 29, 52], [7, 29, 58], [7, 52, 58], [36, 40, 60], [29, 36, 60], [36, 52, 60], [36, 58, 60], [29, 36, 40], [36, 40, 52], [36, 40, 58], [29, 36, 52], [29, 36, 58], [36, 52, 58], [29, 40, 60], [40, 52, 60], [40, 58, 60], [29, 52, 60], [29, 58, 60], [52, 58, 60], [29, 40, 52], [29, 40, 58], [40, 52, 58], [29, 52, 58]], 4: [[29, 52, 56, 58], [29, 52, 60, 66], [29, 58, 60, 66], [52, 58, 60, 66], [29, 52, 58, 66], [29, 34, 40, 52], [29, 34, 40, 58], [34, 40, 52, 58], [29, 34, 52, 58], [29, 58, 60, 62], [52, 58, 60, 62], [29, 40, 52, 62], [29, 40, 58, 62], [40, 52, 58, 62], [29, 52, 58, 62], [7, 40, 58, 60], [7, 29, 52, 60], [7, 29, 58, 60], [7, 52, 58, 60], [7, 29, 40, 52], [7, 29, 40, 58], [7, 40, 52, 58], [7, 29, 52, 58], [29, 36, 40, 60], [36, 40, 52, 60], [36, 40, 58, 60], [29, 36, 52, 60], [29, 36, 58, 60], [36, 52, 58, 60], [29, 36, 40, 52], [29, 36, 40, 58], [36, 40, 52, 58], [29, 36, 52, 58], [29, 40, 52, 60], [29, 40, 58, 60], [40, 52, 58, 60], [29, 52, 58, 60], [29, 40, 52, 58]], 5: [[29, 52, 58, 60, 66], [29, 34, 40, 52, 58], [29, 40, 52, 58, 62], [7, 29, 52, 58, 60], [7, 29, 40, 52, 58], [29, 36, 40, 52, 60], [29, 36, 40, 58, 60], [36, 40, 52, 58, 60], [29, 36, 52, 58, 60], [29, 36, 40, 52, 58], [29, 40, 52, 58, 60]], 6: [[29, 36, 40, 52, 58, 60]]}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "a=TP(data=data,s=s, minSup=3000)\n",
    "\n",
    "print(a.miningResults())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mp0RFbw-RjaU"
   },
   "source": [
    "### Answer questions here:\n",
    "**Why don't we compute support of items while reading data?**  \n",
    "Ta không tính support khi đọc dữ liệu vì sẽ gây tốn chi phí, tính toán những phép tính không cần thiết và làm giảm hiệu suất của thuật toán.  \n",
    "  \n",
    "**why should we do sort**  \n",
    "Để các phần tử trong self.s được sắp xếp theo thứ tự tăng dần của support, giúp xây dựng danh sách các frequent itemset theo thứ tự từ support thấp đến support cao. Việc sắp xếp này đảm bảo frequent itemsets với support thấp được kiểm tra trước, giảm số lần duyệt database và tối ưu hóa thuật toán khai thác dữ liệu.  \n",
    "  \n",
    "**study about python set and its advantages ?**  \n",
    "Khi lưu trữ itemsets trong các thuật toán dạng cây như Apriori và FP, thường sử dụng Python set vì nó có một số lợi ích sau:  \n",
    "1. Không trùng lặp: Set trong Python không cho phép các phần tử trùng lặp, sử dụng set giúp đơn giản hóa quá trình xử lý dữ liệu và đảm bảo tính chính xác trong việc tính toán support, confidence cho các frequent itemsets.   \n",
    "  \n",
    "2. Nhanh và hiệu quả: Set trong Python được cài đặt dưới dạng bảng băm do đó việc truy vấn, thêm, xóa các phần tử trong set được thực hiện nhanh chóng giúp cải thiện hiệu suất của thuật toán khai thác dữ liệu, đặc biệt là trong việc kiểm tra sự tồn tại của một itemset trong tập dữ liệu lớn.  \n",
    "  \n",
    "3. Linh hoạt và dễ sử dụng: Set trong Python cung cấp nhiều hàm hữu ích cho việc xử lý dữ liệu, chẳng hạn như phép hợp (union), phép giao (intersection), phép hiệu (difference), phép kiểm tra tập con (issubset), phép kiểm tra tập phụ thuộc (issuperset),... Điều này giúp đơn giản hóa việc thao tác và xử lý các itemset.  \n",
    "  \n",
    "**After finish implementing the algorithm tell me why should you use this? Instead of delete item directly from search_space and tree.**  \n",
    "1. Để bảo vệ tính toàn vẹn của dữ liệu, trong trường hợp sử dụng search_space và tree trong các bước tiếp theo hoặc để phân tích thêm.  \n",
    "2. Việc giữ lại giúp chúng ta có thể truy xuất hay quan sát tốt hơn, nó có thể hữu ích khi tìm hiểu cách hoạt động của thuật toán và để xác định sự cố hay lỗi tiềm ẩn.  \n",
    "3. Có thể sử dụng lại.  \n",
    "4. Tiết kiệm chi phí.  \n",
    "\n",
    "**Apriori algorithm and Tree Projection, tell me the differences of two algorithms.**  \n",
    "Apriori sử dụng phương pháp kết hợp và cắt bỏ các tập hợp ứng viên dựa trên giá trị support. Thuật toán này sử dụng cấu trúc candidate itemsets để lưu trữ các tập hợp ứng viên và tiếp tục sinh ra các frequent itemsets cho đến khi không còn tập hợp nào thỏa mãn điều kiện support nữa.  \n",
    "  \n",
    "Tree Projection là một phương pháp cải tiến của Apriori. Thuật toán này sử dụng cấu trúc cây để lưu trữ dữ liệu dạng transaction và xây dựng các frequent itemsets. Các frequent itemsets được sinh ra từ cây mà không cần phải thực hiện quá nhiều các phép kết hợp và cắt bỏ như trong Apriori.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NnVm8wYIRjaV"
   },
   "source": [
    "# 3. Churn analysis\n",
    "\n",
    "In this section, you will use frequent itemset mining technique to analyze `churn` dataset (for any purposes). \n",
    "\n",
    "*Remember this dataset is not represented as a transactional database, first thing that you have to do is transforming it into a flat file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readChurn(path):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------\n",
    "        path: path of database D.\n",
    "         \n",
    "    --------------------------\n",
    "    Returns\n",
    "        data: a dictionary for representing database D\n",
    "                 - keys: transaction tids\n",
    "                 - values: itemsets.\n",
    "        s: support of distict items in D.\n",
    "    \"\"\"\n",
    "    data={}\n",
    "    s=defaultdict(lambda: 0) # Initialize a dictionary for storing support of items in I.  \n",
    "    with open(path,'rt') as f:\n",
    "        tid=0;\n",
    "        next(f)\n",
    "        for line in f:\n",
    "            line = line[:-2]\n",
    "            itemset=set(line.split(\",\")) # a python set is a native way for storing an itemset.\n",
    "            for item in itemset:  \n",
    "                s[item]+=1     #Why don't we compute support of items while reading data?\n",
    "            data[tid]= itemset\n",
    "            tid+=1\n",
    "    return data, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataChurn, sChurn= readChurn('churn.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: [['75'], ['127'], ['126'], ['WV'], ['73'], ['124'], ['9'], ['76'], ['8'], ['81'], ['125'], ['79'], ['74'], ['77'], ['122'], ['119'], ['78'], ['82'], ['84'], ['118'], ['80'], ['121'], ['123'], ['120'], ['85'], ['83'], ['116'], ['86'], ['114'], ['111'], ['89'], ['115'], ['117'], ['113'], ['112'], ['90'], ['109'], ['87'], ['110'], ['103'], ['107'], ['93'], ['99'], ['88'], ['96'], ['92'], ['97'], ['101'], ['91'], ['106'], ['100'], ['98'], ['95'], ['104'], ['7'], ['108'], ['94'], ['102'], ['105'], ['6'], ['True'], ['5'], ['4'], ['408'], ['510'], ['3'], ['2'], ['yes'], ['1'], ['415'], ['0'], ['False'], ['no']], 2: []}\n"
     ]
    }
   ],
   "source": [
    "a=TP(data=dataChurn,s=sChurn, minSup=100)\n",
    "\n",
    "print(a.miningResults())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FzxGs7RRjaX"
   },
   "source": [
    "# 4 References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "doYH4biqR_N7"
   },
   "source": [
    "Feel free to send questions to my email address: nnduc@fit.hcmus.edu.vn\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Lab01 - Frequent itemset mining.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
